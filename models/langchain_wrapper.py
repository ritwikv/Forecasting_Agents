"""
LangChain wrapper for Phi-3 model integration
"""
import logging
from typing import Any, Dict, List, Optional, Iterator
from langchain.llms.base import LLM
from langchain.callbacks.manager import CallbackManagerForLLMRun
from pydantic import Field

from .phi3_model import Phi3Model

logger = logging.getLogger(__name__)

class LangChainPhi3Wrapper(LLM):
    """
    LangChain wrapper for Phi-3 model using llama-cpp-python
    """
    
    model_path: Optional[str] = Field(default=None)
    max_tokens: int = Field(default=1024)
    temperature: float = Field(default=0.1)
    top_p: float = Field(default=0.9)
    top_k: int = Field(default=40)
    stop: Optional[List[str]] = Field(default=None)
    
    # Private attributes
    _model: Optional[Phi3Model] = None
    
    class Config:
        """Configuration for this pydantic object."""
        extra = "forbid"
    
    def __init__(self, **kwargs):
        """Initialize the LangChain wrapper"""
        super().__init__(**kwargs)
        self._model = None
    
    @property
    def _llm_type(self) -> str:
        """Return type of LLM."""
        return "phi3_local"
    
    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """Get the identifying parameters."""
        return {
            "model_path": self.model_path,
            "max_tokens": self.max_tokens,
            "temperature": self.temperature,
            "top_p": self.top_p,
            "top_k": self.top_k,
        }
    
    def _get_model(self) -> Phi3Model:
        """Get or create the Phi-3 model instance"""
        if self._model is None:
            self._model = Phi3Model(model_path=self.model_path)
            if not self._model.load_model():
                raise RuntimeError("Failed to load Phi-3 model")
        return self._model
    
    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """
        Call the Phi-3 model and return the output.
        
        Args:
            prompt: The prompt to pass into the model.
            stop: Optional list of stop words to use when generating.
            run_manager: Optional callback manager.
            
        Returns:
            The string generated by the model.
        """
        model = self._get_model()
        
        # Merge stop sequences
        stop_sequences = stop or self.stop or []
        
        # Override parameters with kwargs if provided
        generation_params = {
            'max_tokens': kwargs.get('max_tokens', self.max_tokens),
            'temperature': kwargs.get('temperature', self.temperature),
            'top_p': kwargs.get('top_p', self.top_p),
            'top_k': kwargs.get('top_k', self.top_k),
            'stop': stop_sequences
        }
        
        try:
            result = model.generate(prompt, **generation_params)
            
            # Notify callback manager if available
            if run_manager:
                run_manager.on_llm_new_token(result['text'])
            
            return result['text']
            
        except Exception as e:
            logger.error(f"Error during model generation: {str(e)}")
            raise
    
    def _stream(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> Iterator[str]:
        """
        Stream the output of the model.
        
        Note: This is a simple implementation that returns the full response.
        For true streaming, llama-cpp-python would need to be configured differently.
        """
        result = self._call(prompt, stop, run_manager, **kwargs)
        yield result
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """
        Get performance statistics from the underlying model
        
        Returns:
            Dictionary with performance metrics
        """
        if self._model:
            return self._model.get_performance_stats()
        return {'message': 'Model not initialized'}
    
    def clear_performance_stats(self):
        """Clear performance tracking data"""
        if self._model:
            self._model.clear_performance_stats()

class ChatPhi3Wrapper:
    """
    Chat-specific wrapper for Phi-3 model with conversation management
    """
    
    def __init__(self, model_path: Optional[str] = None, **kwargs):
        """
        Initialize chat wrapper
        
        Args:
            model_path: Path to GGUF model file
            **kwargs: Additional parameters for the model
        """
        self.model = Phi3Model(model_path=model_path)
        self.conversation_history = []
        self.system_message = None
        
        # Model parameters
        self.generation_params = {
            'max_tokens': kwargs.get('max_tokens', 1024),
            'temperature': kwargs.get('temperature', 0.1),
            'top_p': kwargs.get('top_p', 0.9),
            'top_k': kwargs.get('top_k', 40),
            'stop': kwargs.get('stop', ['<|end|>', '<|user|>'])
        }
    
    def set_system_message(self, message: str):
        """
        Set system message for the conversation
        
        Args:
            message: System message content
        """
        self.system_message = message
    
    def chat(self, user_message: str, **kwargs) -> Dict[str, Any]:
        """
        Send a message and get response
        
        Args:
            user_message: User's message
            **kwargs: Override generation parameters
            
        Returns:
            Dictionary with response and metadata
        """
        # Build conversation messages
        messages = []
        
        # Add system message if set
        if self.system_message:
            messages.append({'role': 'system', 'content': self.system_message})
        
        # Add conversation history
        messages.extend(self.conversation_history)
        
        # Add current user message
        messages.append({'role': 'user', 'content': user_message})
        
        # Override generation parameters if provided
        params = self.generation_params.copy()
        params.update(kwargs)
        
        # Generate response
        result = self.model.chat(messages, **params)
        
        # Update conversation history
        self.conversation_history.append({'role': 'user', 'content': user_message})
        self.conversation_history.append({'role': 'assistant', 'content': result['text']})
        
        return result
    
    def clear_conversation(self):
        """Clear conversation history"""
        self.conversation_history.clear()
    
    def get_conversation_history(self) -> List[Dict[str, str]]:
        """
        Get conversation history
        
        Returns:
            List of message dictionaries
        """
        return self.conversation_history.copy()
    
    def load_model(self) -> bool:
        """Load the underlying model"""
        return self.model.load_model()
    
    def unload_model(self):
        """Unload the underlying model"""
        self.model.unload_model()

